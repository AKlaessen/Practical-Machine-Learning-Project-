---
title: 'Peer-graded Assignment: Prediction Assignment Writeup'
author: "L.A. Klaessen"
date: "2018 M06 6"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

### Background Information

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

### Goal

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

## Data Processing

### Initial set-up

First the libraries which are needed for the project have to be loaded. The following libraries are used.
```{r library, warning = FALSE, message = FALSE}
library(rpart);
library(rpart.plot);
library(RColorBrewer);
library(rattle);
library(randomForest);
library(caret);
```
If another user want to reproduce the same results as this project, the same seed has to be chosen. The following seed in this project can be found below.
```{r seed, message = FALSE} 
set.seed(123)
```

### Downloading data

The URL of the training en test data for this project are:

```{r trainURL}
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```
The data contains several types of missing data in the form of "NA", "#DIV/0!" or an empty space. The data is loaded into the memory directly. The test data is used as a validation data set for the predicting model.

```{r loadData}
trainData      <- read.csv(url(trainURL), na.strings=c("NA","#DIV/0!",""))
validationData <- read.csv(url(testURL), na.strings=c("NA","#DIV/0!",""))
```

### Cleaning Data

Both data sets contains many columns with NA values. NA values don't contribute to preding any values, so these columns are omitted in the data sets. Also the first 7 columns contain user information and time information, which is not usefull for the predicting models. So the first 7 columns are omitted as well.

```{r cleaning}
trainData <- trainData[, colSums(is.na(trainData)) == 0]
validationData <- validationData[, colSums(is.na(validationData)) == 0]

trainData <- trainData[, -c(1:7)]
validationData <- validationData[, -c(1:7)]
str(trainData)
```

### Splitting data

The training data is split into a train set and test set. The train set is used to build the predicting model and the test set is used to compute ou-of-sample errors. For this purpose, 80% of the training data is used for the train set, and therefore 20% of the training data is used as test set,

```{r splitSet}
inTrain <- createDataPartition(trainData$classe, p=0.80, list=FALSE)
trainSet <- trainData[inTrain, ]
testSet <- trainData[-inTrain, ]
dim(trainSet); dim(testSet)
```


## Prediction models

The selected models for this project are the decission tree and random forest model.

### Decission tree

In both models, the k-fold cross validation is set on 5, to save some computing time. 
```{r DT}
trControl <- trainControl(method="cv", number=5)
model_DT  <- train(classe~., data=trainSet, method="rpart", trControl=trControl)
```

```{r plotDT}
fancyRpartPlot(model_DT$finalModel)
```

```{r predictionsDT}
predictions_DT <- predict(model_DT, newdata=testSet)
confMatDT <- confusionMatrix(predictions_DT , testSet$classe)
confMatDT

```

From the confusion matrix, it can be seen that the accuracy is 0.4973. This means that the out-of-sample error rate is 0.5027. Due to the high out-of-sample error rate the decission tree is not a suitable model to predict the classe variables.

### Random forest

```{r FF}
model_RF <- train(classe ~ ., data = trainSet, method = "rf", trControl = trControl)
print(model_RF, digits = 4)
```

```{r }
predictions_RF <- predict(model_RF,newdata=testSet)

confMatRF <- confusionMatrix(testSet$classe,predictions_RF)
confMatRF
```

From the confusion matrix, it can be seen that the accuracy is 0.9936. This means that the out-of-sample error rate is 0.0064. The random forest model performs much better than the decission tree model. Therefore the random forest model is used to predict the classe variables for the 20 cases.

## Conclusion

From the selected models, it is shown that the random forest model has the highest accuracy. Therefore the random forest model is used to predict the classe variables from the validation set.

```{r validation}
predict(model_RF, validationData)

```
